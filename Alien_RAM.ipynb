{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Acrobot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZASuGa_Xbzd",
        "colab_type": "text"
      },
      "source": [
        "## **Use TF-Agent to Train a RL on Acrobot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZX49wH_XmVr",
        "colab_type": "text"
      },
      "source": [
        "# **Set Up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDO_pjLTXkPy",
        "colab_type": "code",
        "outputId": "a1d81cf0-06bd-4e05-d296-8c9624d06040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install xvfb\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents-nightly\n",
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 783 kB in 1s (979 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 134983 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting gym==0.10.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.17.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.11) (1.3.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.11) (2019.11.28)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.11) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-cp36-none-any.whl size=1588314 sha256=6c49f173d470914602029c31587066d7cd770a42aef5bb2c51159901ab88c4c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.15.4\n",
            "    Uninstalling gym-0.15.4:\n",
            "      Successfully uninstalled gym-0.15.4\n",
            "Successfully installed gym-0.10.11\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (1.17.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio==2.4.0) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio==2.4.0) (0.46)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp36-none-any.whl size=3303880 sha256=a7f2c429d79ee0bda7a441d410aa2ba972926189816bcabdee62eda94583f608\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from PILLOW) (0.46)\n",
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.7 pyvirtualdisplay-0.2.4\n",
            "Collecting tf-agents-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/48/76b901c22b282dc02f2bb6f8e07a054b1fd6357b1ff59c35302d04da006a/tf_agents_nightly-0.2.0.dev20191210-py2.py3-none-any.whl (841kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (1.17.4)\n",
            "Collecting gin-config==0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/be/c984b1c8a7ba1c385b32bf39c7a225cd9f713d49705898309d01b60fd0e7/gin_config-0.1.3-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hCollecting tfp-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/50/64b107ce5852f6952bbf86d45632c23764ebfff3dd514d0a910246c220ec/tfp_nightly-0.9.0.dev20191210-py2.py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 31.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.1)\n",
            "Requirement already satisfied: gast>=0.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (0.2.2)\n",
            "Installing collected packages: gin-config, tfp-nightly, tf-agents-nightly\n",
            "  Found existing installation: gin-config 0.2.1\n",
            "    Uninstalling gin-config-0.2.1:\n",
            "      Successfully uninstalled gin-config-0.2.1\n",
            "Successfully installed gin-config-0.1.3 tf-agents-nightly-0.2.0.dev20191210 tfp-nightly-0.9.0.dev20191210\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jES_Q4RUXtml",
        "colab_type": "code",
        "outputId": "603f5951-faf3-440e-a8db-a71e725e1e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5nfcP8QaLK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import network\n",
        "\n",
        "from tf_agents.policies import py_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import scripted_py_policy\n",
        "\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import actor_policy\n",
        "from tf_agents.policies import q_policy\n",
        "from tf_agents.policies import greedy_policy\n",
        "\n",
        "from tf_agents.trajectories import time_step as ts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7O9fMJyhqob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 200000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 500  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UhZk5LWX1WO",
        "colab_type": "text"
      },
      "source": [
        "# **Environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAMZcMvBZ3ye",
        "colab_type": "text"
      },
      "source": [
        "Acrobot is a 2-link pendulum with only the second joint actuated. Initially, both links point downwards. The goal is to swing the end-effector at a height at least the length of one link above the base. Both links can swing freely and can pass by each other, i.e., they don't collide when they have the same angle.\n",
        "\n",
        "The obersvations consists of the sin() and cos() of the two rotational joint angles and the joint angular velocities : [cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n",
        "\n",
        "The reward is -1 as long as the end-effector does not reach a certain height."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALtBWh14X4tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = 'Alien-ram-v0'\n",
        "env = suite_gym.load(env_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wugFiZCPYpT9",
        "colab_type": "code",
        "outputId": "c6975d7f-759b-4e1c-d098-5f56ba36a409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAEsklEQVR4nO3dMXLTQBiGYYXhGOlo\naFKQyeQEuYM7V5QUKTlCSg5ARccNUnACTwa3aeg4CIVmPB4SKZJ2/9Xuq/epMoaIDR8flle/5a6T\nJElSdhdDv3B1/fjq4x9+P4UtRm/7c30z6/e/C1qHKmHAcAYMZ8BwBgxnwHAGDPc+5ZvnvibTMil7\nDzYYzoDhDBhucwEf989rL6GozQW8NZkD3h0u8x7wJL15x/1zf5DTF1uQ9DKpLZ9+fOy67rh/7r/Y\niMwN/nn799XH05s9lMp2urjM5p6DN1XfrljANnst2AY/dF/WXkIVVg54qNkZjvzjV9CR24JtsHoG\nDGfAcCEbHblmp4euN681mx29nojr6zYYLnODd4fL/06M5/6rHGrDbn/36olx9FTJ3Hbm+nlzscFw\nhfaiMxzZ17WL2GC48MuF0c8xtb3bsbb12GC48AZHnOXmOlPNtbaU43gWrSQGDGfAcAYMZ8BwzcxF\n7/Z3QUdms8FwzcxFD+1F2+xxNhiu+blomz3OBsM5Fw1ng+EMGM6A4VZ7A3jEddBc13drXttcNhgu\nfC56iponImpe2xQ2GK6ZvWgtY4PhCp1FT39izvW8NfReprmi35sUzQbDhTS4hnnj8/rOPU7JdwhG\nN94Gw20u4N3+blOXijcXcLexK4lbDNgGw9ng5ZqYi7bB4mhmL9rpyWVsMNzKc9EZjmyzR9lgOGzA\nm3otNAIbsHoGDLehz03qbe2/7qSAp1wHjbjeWdv9olN4nywlCfkE8Ih/la3cLzpFxN+nDYYrFHDc\nXrTG2WC48LPo3vhedMpzTA3v/zlX289ig+Euhn7h6vqx5DoUxAbDGTCcAcMZMJwBwzUzF308fA86\nMpsNhmtmLvrT7edXH7fZ42wwXPP3i7bZ42wwnHvRcDYYzoDhDBhu9kRHrju/lbwn89zPD65hbXOP\nM8QGw1UxF13zPZlLrs25aM3WzFy0lrHBcM3PRQ85Hr4PbVPPUnLu2rlozRZyFh39vVOk1LfkXdun\n/FmeRWuQAcMZMJwBwxkwnHPRcDYYzrloOBsM51w0nA2GWznguDvBZ7mUBGCD4QwYzoDhQm4Inmsy\nIXpuuYapxynHT2GD4TI3eHe4/O/EONenaQ/NWNV2v+iSnx4+hQ2GK7QXnc7XtcvYYLjwj9WJfo4h\n3S86gg2GC2lw9DvySs4tz1Xb2mwwnAHDGTCcAcMZMJxz0XA2GM65aDgbDOdcNJwNhnMuGs4Gwxkw\nnAHDrTYXnev+UNF3vC05g+1ctGYrOhe9VqtqPo5z0UrSzFy0lrHBcCvPRb98zp7+vbnW0O5xprDB\ncCvPRb+sb8RrwZSz31zrWWte2gbDGTCcAcMZMJwBwzkXDWeD4ZyLhrPBcNi5aPVsMBx2Llo9Gwxn\nwHAGDFfoE8DrR/pZztlgOAOGCx+6i3bz7evp66f7hxVXUqe2G9yn+3T/0Ed7HrZ6bQesNxkwnAHD\nGTBc2wGfzq1OZ1trr6g6zb9MMtRxbTdYbzJgOAOGSw34dIIz8XEVlhrw0B6he4eVyNDg0xfncQ49\nrsLyNLg72/Eff1yF5WnwywjdeahE6kbHUIRGK0mSJEmSJEmSJEmStNQ/7LzUVf4jTwoAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7F318DD5B940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUhWJMZ2Ywxh",
        "colab_type": "code",
        "outputId": "eba17aa9-2485-418e-e280-664116375ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print('Observation Spec:', env.time_step_spec().observation)\n",
        "print('Reward Spec:', env.time_step_spec().reward)\n",
        "print('Action Spec:', env.action_spec())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Spec: BoundedArraySpec(shape=(128,), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n",
            "Reward Spec: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
            "Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVFhib6bbgw6",
        "colab_type": "text"
      },
      "source": [
        "Try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AplI2YVPbh-j",
        "colab_type": "code",
        "outputId": "9ab85faf-5b2f-47e5-fdbc-52200ba8c189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = 2\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)\n",
        "\n",
        "num = 0\n",
        "while not next_time_step.is_last():\n",
        "  next_time_step = env.step(action)\n",
        "  num = num + 1\n",
        "\n",
        "# stop after 500 steps\n",
        "print(num)\n",
        "print(next_time_step)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time step:\n",
            "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([  1,   0,  71, 128,   0, 128, 240, 128, 240, 128, 240, 128, 240,\n",
            "         0, 240, 255,   0, 136, 240,  26, 241,   1,   2,   6,   5,   3,\n",
            "         6,   0,   1,   6, 255,   0,   0,   0,   3,  87,  87,  87, 139,\n",
            "       135,   0,   0,   0,   0,   0,  67,  90,   0,   0,   0,   0,   0,\n",
            "        60,  59,  60, 112, 112, 112, 116, 147, 116,   0,  11,   0,   3,\n",
            "       255,  14, 255, 148, 190,  24, 255, 144, 255,  12, 255,  68, 127,\n",
            "       255, 192, 255, 164, 244,  96, 255,  36, 255, 192, 255, 136, 248,\n",
            "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   3,\n",
            "         0,   0,  25,   0,   0,   0,   0,   0,  11,   0,   0,   0, 255,\n",
            "       190,  30,  94, 142,  78,  30, 142,  96, 132, 254, 248], dtype=uint8))\n",
            "Next time step:\n",
            "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([  0,   0,  73, 128,   0, 128, 240, 128, 240, 128, 240, 128, 240,\n",
            "         0, 240, 255,   0, 136, 240,  26, 241,   1,   2,   6,   5,   3,\n",
            "         6,   0,   1,   6, 255,   0,   0,   0,   3,  87,  87,  87, 103,\n",
            "       135,   0,   0,   0,   0,   0,  67,  90,   0,   0,   0,   0,   0,\n",
            "        60,  59,  60, 112, 112, 112, 116, 147, 116,   0,  13,   0,   3,\n",
            "       255,  14, 255, 148, 190,  24, 255, 144, 255,  12, 255,  68, 127,\n",
            "       255, 192, 255, 164, 244,  96, 255,  36, 255, 192, 255, 136, 248,\n",
            "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   3,\n",
            "         0,   0,  24,   0,   0,   0,   0,   0,  13,   0,   0,   0, 255,\n",
            "       190,  30,  94, 142,  78,  30, 142,  96, 132, 254, 248], dtype=uint8))\n",
            "541\n",
            "TimeStep(step_type=array(2, dtype=int32), reward=array(0., dtype=float32), discount=array(0., dtype=float32), observation=array([192,   0, 163, 128,   3, 128,   6, 128, 102, 128, 240, 128,   0,\n",
            "         0, 240,  96,  96, 136, 240,  26, 241,   1,   2,   6,   5,   3,\n",
            "         6,   0,   0,   4,   0,  67, 141, 120,   3,  79,  79,  79,  72,\n",
            "       119,   0,   0,   0,   0,   0,  67,  90,   0,   0,  60,  68,  58,\n",
            "        60,  59,  60, 116, 252, 163, 116, 147, 116,   0, 102,   3,   0,\n",
            "       255,  14, 255, 148, 190,  24, 255, 144, 255,  12, 255,  68, 127,\n",
            "       255, 192, 255, 164, 244,  96, 255,  36, 255, 192, 255, 136, 248,\n",
            "         0,   0,   0,   0,   0,   0,   0, 152, 132,  40, 255,   4,   3,\n",
            "         0,   0,  30,   0,   0,   0,   0,   0,  11,   0,   4,   0, 255,\n",
            "       190,  30,  94, 142,  78,  30, 142,   0, 132,  23, 247], dtype=uint8))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALjSYaNFcZta",
        "colab_type": "text"
      },
      "source": [
        "Instantiate two environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne2ZOlDQcbzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6dufwvciv6",
        "colab_type": "text"
      },
      "source": [
        "# **Agent**-DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q58-8ww5hY0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NN Q Function\n",
        "fc_layer_params = (100,100)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZvIFLGEhhfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DdqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    n_step_update=2)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahvWwfysiA2U",
        "colab_type": "text"
      },
      "source": [
        "# **Policies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zte4rQ4Oh5g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                          train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ6aipb9ibdF",
        "colab_type": "text"
      },
      "source": [
        "# **Metrix and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdZHMA4Fieuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlCAGqlDjyoV",
        "colab_type": "code",
        "outputId": "8d9f6547-3dde-4405-efd9-13c5571c3aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8C26zQORRUM",
        "colab_type": "text"
      },
      "source": [
        "# **Replay Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6mNw0g7RZM6",
        "colab_type": "code",
        "outputId": "6609d9da-8067-4a3b-d632-1735c2a78650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "agent.collect_data_spec\n",
        "agent.collect_data_spec._fields"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz5V-JZNUGFq",
        "colab_type": "text"
      },
      "source": [
        "# **Data Collecting Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE4XLY4-UJkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9SwxZHe0Bmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_step(train_env, agent.collect_policy, replay_buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWVPeGb2VPGH",
        "colab_type": "code",
        "outputId": "3a29af0e-1fe2-46fc-8868-1e234c7a4ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=3).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.IteratorV2 object at 0x7f3142c6f198>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9C57RZj5IpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37ff3551-8541-4e4e-f51a-13643e8c2ff9"
      },
      "source": [
        "experience, unused_info = next(iterator)\n",
        "#agent.train(experience, weights=[1,1])\n",
        "experience"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(step_type=<tf.Tensor: id=610075, shape=(64, 3), dtype=int32, numpy=\n",
              "array([[1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 1, 1],\n",
              "       [1, 1, 1]], dtype=int32)>, observation=<tf.Tensor: id=610076, shape=(64, 3, 128), dtype=uint8, numpy=\n",
              "array([[[  0,   0,  74, ..., 132, 254, 248],\n",
              "        [  0,   0,  77, ..., 132, 254, 248],\n",
              "        [  0,   0,  81, ..., 132, 254, 248]],\n",
              "\n",
              "       [[  1,   0,  71, ..., 132, 254, 248],\n",
              "        [  0,   0,  74, ..., 132, 254, 248],\n",
              "        [  0,   0,  77, ..., 132, 254, 248]],\n",
              "\n",
              "       [[  0,   0,  77, ..., 132, 254, 248],\n",
              "        [  0,   0,  81, ..., 132, 254, 248],\n",
              "        [  0,   0,  84, ..., 132, 254, 248]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[  0,   0,  77, ..., 132, 254, 248],\n",
              "        [  0,   0,  81, ..., 132, 254, 248],\n",
              "        [  0,   0,  84, ..., 132, 254, 248]],\n",
              "\n",
              "       [[  1,   0,  71, ..., 132, 254, 248],\n",
              "        [  0,   0,  74, ..., 132, 254, 248],\n",
              "        [  0,   0,  77, ..., 132, 254, 248]],\n",
              "\n",
              "       [[  0,   0,  74, ..., 132, 254, 248],\n",
              "        [  0,   0,  77, ..., 132, 254, 248],\n",
              "        [  0,   0,  81, ..., 132, 254, 248]]], dtype=uint8)>, action=<tf.Tensor: id=610077, shape=(64, 3), dtype=int64, numpy=\n",
              "array([[14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [14, 14, 14],\n",
              "       [ 9, 14, 14],\n",
              "       [14, 14, 14]])>, policy_info=(), next_step_type=<tf.Tensor: id=610078, shape=(64, 3), dtype=int32, numpy=\n",
              "array([[1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1]], dtype=int32)>, reward=<tf.Tensor: id=610079, shape=(64, 3), dtype=float32, numpy=\n",
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]], dtype=float32)>, discount=<tf.Tensor: id=610080, shape=(64, 3), dtype=float32, numpy=\n",
              "array([[1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs0lOVYmUL4S",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxq4BzcvUNoJ",
        "colab_type": "code",
        "outputId": "7a65e303-e0bc-47cd-ccbc-bf1e31e60e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "  \n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "  \n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 500: loss = 4263675166720.0\n",
            "step = 1000: loss = 29908669562880.0\n",
            "step = 1000: Average Return = 0.0\n",
            "step = 1500: loss = 102974969872384.0\n",
            "step = 2000: loss = 590187495161856.0\n",
            "step = 2000: Average Return = 30.0\n",
            "step = 2500: loss = 535131651571712.0\n",
            "step = 3000: loss = 2098612557316096.0\n",
            "step = 3000: Average Return = 30.0\n",
            "step = 3500: loss = 2772981210152960.0\n",
            "step = 4000: loss = 3310303093719040.0\n",
            "step = 4000: Average Return = 0.0\n",
            "step = 4500: loss = 3882387905708032.0\n",
            "step = 5000: loss = 1.665314470274007e+17\n",
            "step = 5000: Average Return = 30.0\n",
            "step = 5500: loss = 1.0982828376326144e+16\n",
            "step = 6000: loss = 8684267643076608.0\n",
            "step = 6000: Average Return = 30.0\n",
            "step = 6500: loss = 1.1986441974317056e+16\n",
            "step = 7000: loss = 1.0618209275215872e+16\n",
            "step = 7000: Average Return = 0.0\n",
            "step = 7500: loss = 1.7270147249078272e+16\n",
            "step = 8000: loss = 2.0052895214862336e+16\n",
            "step = 8000: Average Return = 0.0\n",
            "step = 8500: loss = 1.9439855205351424e+16\n",
            "step = 9000: loss = 2.475212416011469e+16\n",
            "step = 9000: Average Return = 30.0\n",
            "step = 9500: loss = 2.0312442236043264e+16\n",
            "step = 10000: loss = 1.652430148206592e+16\n",
            "step = 10000: Average Return = 30.0\n",
            "step = 10500: loss = 1.865187473542349e+16\n",
            "step = 11000: loss = 2.210550733026099e+16\n",
            "step = 11000: Average Return = 30.0\n",
            "step = 11500: loss = 3.258201704444723e+16\n",
            "step = 12000: loss = 2.651325187751936e+16\n",
            "step = 12000: Average Return = 30.0\n",
            "step = 12500: loss = 1.3903992400642048e+16\n",
            "step = 13000: loss = 1.962545792208077e+16\n",
            "step = 13000: Average Return = 30.0\n",
            "step = 13500: loss = 3.360031513562317e+16\n",
            "step = 14000: loss = 2.3907209603710976e+16\n",
            "step = 14000: Average Return = 30.0\n",
            "step = 14500: loss = 3.557509385368371e+16\n",
            "step = 15000: loss = 3.5423606062186496e+16\n",
            "step = 15000: Average Return = 0.0\n",
            "step = 15500: loss = 1.2680168699739505e+18\n",
            "step = 16000: loss = 2.505489787964621e+16\n",
            "step = 16000: Average Return = 0.0\n",
            "step = 16500: loss = 2.3655341984055296e+16\n",
            "step = 17000: loss = 4.475025209950208e+16\n",
            "step = 17000: Average Return = 0.0\n",
            "step = 17500: loss = 4.382095002566656e+16\n",
            "step = 18000: loss = 5.0506667917312e+16\n",
            "step = 18000: Average Return = 0.0\n",
            "step = 18500: loss = 3.139224667894579e+16\n",
            "step = 19000: loss = 2.849883458581299e+16\n",
            "step = 19000: Average Return = 30.0\n",
            "step = 19500: loss = 4.143545217502413e+16\n",
            "step = 20000: loss = 2.327789381563187e+16\n",
            "step = 20000: Average Return = 100.0\n",
            "step = 20500: loss = 1.5207760389409341e+18\n",
            "step = 21000: loss = 2.0974747667922944e+16\n",
            "step = 21000: Average Return = 0.0\n",
            "step = 21500: loss = 4.294918333372826e+16\n",
            "step = 22000: loss = 2.230072433128243e+16\n",
            "step = 22000: Average Return = 0.0\n",
            "step = 22500: loss = 5.296627542864691e+16\n",
            "step = 23000: loss = 2.103110193381376e+16\n",
            "step = 23000: Average Return = 0.0\n",
            "step = 23500: loss = 1.552318828518572e+18\n",
            "step = 24000: loss = 3.319005556454195e+16\n",
            "step = 24000: Average Return = 100.0\n",
            "step = 24500: loss = 1.909915907207987e+16\n",
            "step = 25000: loss = 3.638511609577472e+16\n",
            "step = 25000: Average Return = 0.0\n",
            "step = 25500: loss = 2.365395470961869e+16\n",
            "step = 26000: loss = 3.335198656901939e+16\n",
            "step = 26000: Average Return = 100.0\n",
            "step = 26500: loss = 1.8605117573955584e+16\n",
            "step = 27000: loss = 1.5331573645034127e+18\n",
            "step = 27000: Average Return = 140.0\n",
            "step = 27500: loss = 3.1735992238997504e+16\n",
            "step = 28000: loss = 2.9327030982017024e+16\n",
            "step = 28000: Average Return = 0.0\n",
            "step = 28500: loss = 1.5835584277648507e+18\n",
            "step = 29000: loss = 2.96758617833472e+16\n",
            "step = 29000: Average Return = 100.0\n",
            "step = 29500: loss = 2.5363611538948096e+16\n",
            "step = 30000: loss = 1.7696627837894656e+16\n",
            "step = 30000: Average Return = 100.0\n",
            "step = 30500: loss = 1.2811020204171592e+18\n",
            "step = 31000: loss = 1.465278601845801e+18\n",
            "step = 31000: Average Return = 140.0\n",
            "step = 31500: loss = 1.3835631725473956e+18\n",
            "step = 32000: loss = 1.195774192910336e+16\n",
            "step = 32000: Average Return = 100.0\n",
            "step = 32500: loss = 2.2411328329089024e+16\n",
            "step = 33000: loss = 4.283002376106803e+16\n",
            "step = 33000: Average Return = 0.0\n",
            "step = 33500: loss = 2.336667078964019e+16\n",
            "step = 34000: loss = 3.284938305359053e+16\n",
            "step = 34000: Average Return = 0.0\n",
            "step = 34500: loss = 2.610199802150912e+16\n",
            "step = 35000: loss = 1.5428041144532992e+16\n",
            "step = 35000: Average Return = 100.0\n",
            "step = 35500: loss = 2.005349436280013e+16\n",
            "step = 36000: loss = 1.1686990841485722e+18\n",
            "step = 36000: Average Return = 0.0\n",
            "step = 36500: loss = 2.4829725629218816e+16\n",
            "step = 37000: loss = 1.1318805414281216e+16\n",
            "step = 37000: Average Return = 100.0\n",
            "step = 37500: loss = 1.4855482275528704e+16\n",
            "step = 38000: loss = 1.1819930387218432e+16\n",
            "step = 38000: Average Return = 140.0\n",
            "step = 38500: loss = 2.901107386037043e+16\n",
            "step = 39000: loss = 2.729888729530368e+16\n",
            "step = 39000: Average Return = 100.0\n",
            "step = 39500: loss = 8719635088146432.0\n",
            "step = 40000: loss = 9.146726593110999e+17\n",
            "step = 40000: Average Return = 30.0\n",
            "step = 40500: loss = 9347543200694272.0\n",
            "step = 41000: loss = 2.879085370723533e+16\n",
            "step = 41000: Average Return = 30.0\n",
            "step = 41500: loss = 1.1213914796720128e+16\n",
            "step = 42000: loss = 2.6388553944530944e+18\n",
            "step = 42000: Average Return = 0.0\n",
            "step = 42500: loss = 1.856297750233088e+16\n",
            "step = 43000: loss = 1.5417791849326182e+18\n",
            "step = 43000: Average Return = 140.0\n",
            "step = 43500: loss = 6064518518013952.0\n",
            "step = 44000: loss = 9369667650977792.0\n",
            "step = 44000: Average Return = 0.0\n",
            "step = 44500: loss = 1.953778475466752e+16\n",
            "step = 45000: loss = 9000392805318656.0\n",
            "step = 45000: Average Return = 30.0\n",
            "step = 45500: loss = 2.928361315762176e+16\n",
            "step = 46000: loss = 2.238995657182413e+16\n",
            "step = 46000: Average Return = 140.0\n",
            "step = 46500: loss = 5.373346653887529e+17\n",
            "step = 47000: loss = 2.516129280950272e+16\n",
            "step = 47000: Average Return = 140.0\n",
            "step = 47500: loss = 1.27102040932352e+16\n",
            "step = 48000: loss = 7706225743495168.0\n",
            "step = 48000: Average Return = 100.0\n",
            "step = 48500: loss = 5.827955198273782e+17\n",
            "step = 49000: loss = 8858812966502400.0\n",
            "step = 49000: Average Return = 0.0\n",
            "step = 49500: loss = 6448265255976960.0\n",
            "step = 50000: loss = 1.454628113743872e+16\n",
            "step = 50000: Average Return = 100.0\n",
            "step = 50500: loss = 1.907819318922445e+16\n",
            "step = 51000: loss = 7267488928628736.0\n",
            "step = 51000: Average Return = 0.0\n",
            "step = 51500: loss = 1.4576639040028672e+16\n",
            "step = 52000: loss = 5705765268488192.0\n",
            "step = 52000: Average Return = 100.0\n",
            "step = 52500: loss = 1.6291885769293824e+16\n",
            "step = 53000: loss = 1.130498743074816e+16\n",
            "step = 53000: Average Return = 0.0\n",
            "step = 53500: loss = 6484316137717760.0\n",
            "step = 54000: loss = 7882934656696320.0\n",
            "step = 54000: Average Return = 30.0\n",
            "step = 54500: loss = 1.10683454701568e+16\n",
            "step = 55000: loss = 2.734212473107251e+16\n",
            "step = 55000: Average Return = 140.0\n",
            "step = 55500: loss = 3.224896810044621e+16\n",
            "step = 56000: loss = 2.0845297353621504e+16\n",
            "step = 56000: Average Return = 30.0\n",
            "step = 56500: loss = 2.445228819821363e+16\n",
            "step = 57000: loss = 4394441959800832.0\n",
            "step = 57000: Average Return = 311.0\n",
            "step = 57500: loss = 8234955008114688.0\n",
            "step = 58000: loss = 1.9533378118221824e+16\n",
            "step = 58000: Average Return = 0.0\n",
            "step = 58500: loss = 4050011620900864.0\n",
            "step = 59000: loss = 1.8399161007210496e+16\n",
            "step = 59000: Average Return = 0.0\n",
            "step = 59500: loss = 1.4924873914646528e+16\n",
            "step = 60000: loss = 3677641982869504.0\n",
            "step = 60000: Average Return = 100.0\n",
            "step = 60500: loss = 1.9551618844327936e+16\n",
            "step = 61000: loss = 1821886203822080.0\n",
            "step = 61000: Average Return = 100.0\n",
            "step = 61500: loss = 3.875841194432922e+16\n",
            "step = 62000: loss = 9311905709555712.0\n",
            "step = 62000: Average Return = 30.0\n",
            "step = 62500: loss = 8707712795803648.0\n",
            "step = 63000: loss = 1.5626407000827494e+17\n",
            "step = 63000: Average Return = 140.0\n",
            "step = 63500: loss = 3770246578044928.0\n",
            "step = 64000: loss = 6820920240898048.0\n",
            "step = 64000: Average Return = 100.0\n",
            "step = 64500: loss = 1.4095238704398336e+16\n",
            "step = 65000: loss = 1.2940115840073728e+16\n",
            "step = 65000: Average Return = 0.0\n",
            "step = 65500: loss = 9568391492796416.0\n",
            "step = 66000: loss = 7965007723626496.0\n",
            "step = 66000: Average Return = 0.0\n",
            "step = 66500: loss = 2.053081125827379e+16\n",
            "step = 67000: loss = 2.502406860439552e+16\n",
            "step = 67000: Average Return = 100.0\n",
            "step = 67500: loss = 1.313664108813353e+17\n",
            "step = 68000: loss = 4356197423513600.0\n",
            "step = 68000: Average Return = 0.0\n",
            "step = 68500: loss = 1.512728283840512e+16\n",
            "step = 69000: loss = 1.320763603854295e+17\n",
            "step = 69000: Average Return = 100.0\n",
            "step = 69500: loss = 2412118594813952.0\n",
            "step = 70000: loss = 1642458576322560.0\n",
            "step = 70000: Average Return = 140.0\n",
            "step = 70500: loss = 5704578783772672.0\n",
            "step = 71000: loss = 1.1268220363210752e+16\n",
            "step = 71000: Average Return = 0.0\n",
            "step = 71500: loss = 1.2340087433986048e+16\n",
            "step = 72000: loss = 2733578643308544.0\n",
            "step = 72000: Average Return = 30.0\n",
            "step = 72500: loss = 3964676794744832.0\n",
            "step = 73000: loss = 1.1809003990417408e+16\n",
            "step = 73000: Average Return = 0.0\n",
            "step = 73500: loss = 1.21905551410987e+17\n",
            "step = 74000: loss = 8374450714050560.0\n",
            "step = 74000: Average Return = 0.0\n",
            "step = 74500: loss = 5417663157239808.0\n",
            "step = 75000: loss = 1466914136129536.0\n",
            "step = 75000: Average Return = 100.0\n",
            "step = 75500: loss = 6911753430499328.0\n",
            "step = 76000: loss = 3786168827117568.0\n",
            "step = 76000: Average Return = 140.0\n",
            "step = 76500: loss = 4450047827640320.0\n",
            "step = 77000: loss = 1393571932405760.0\n",
            "step = 77000: Average Return = 30.0\n",
            "step = 77500: loss = 1887885624082432.0\n",
            "step = 78000: loss = 2002817472528384.0\n",
            "step = 78000: Average Return = 140.0\n",
            "step = 78500: loss = 5716606839685120.0\n",
            "step = 79000: loss = 4003933601136640.0\n",
            "step = 79000: Average Return = 0.0\n",
            "step = 79500: loss = 1681467482570752.0\n",
            "step = 80000: loss = 4043326772740096.0\n",
            "step = 80000: Average Return = 100.0\n",
            "step = 80500: loss = 2.630245273514803e+16\n",
            "step = 81000: loss = 1662527482101760.0\n",
            "step = 81000: Average Return = 0.0\n",
            "step = 81500: loss = 6.736185904857088e+16\n",
            "step = 82000: loss = 4.566167422448435e+16\n",
            "step = 82000: Average Return = 30.0\n",
            "step = 82500: loss = 6542960694919168.0\n",
            "step = 83000: loss = 2585278791286784.0\n",
            "step = 83000: Average Return = 0.0\n",
            "step = 83500: loss = 2037615431778304.0\n",
            "step = 84000: loss = 1366763014979584.0\n",
            "step = 84000: Average Return = 100.0\n",
            "step = 84500: loss = 1570851002515456.0\n",
            "step = 85000: loss = 1627798913417216.0\n",
            "step = 85000: Average Return = 100.0\n",
            "step = 85500: loss = 1949535987302400.0\n",
            "step = 86000: loss = 575856195928064.0\n",
            "step = 86000: Average Return = 100.0\n",
            "step = 86500: loss = 5202517507964928.0\n",
            "step = 87000: loss = 1414864098557952.0\n",
            "step = 87000: Average Return = 0.0\n",
            "step = 87500: loss = 1718252669501440.0\n",
            "step = 88000: loss = 762972083322880.0\n",
            "step = 88000: Average Return = 30.0\n",
            "step = 88500: loss = 1031867771912192.0\n",
            "step = 89000: loss = 4514677421768704.0\n",
            "step = 89000: Average Return = 100.0\n",
            "step = 89500: loss = 3809103415607296.0\n",
            "step = 90000: loss = 1779493131780096.0\n",
            "step = 90000: Average Return = 99.0\n",
            "step = 90500: loss = 1134298446954496.0\n",
            "step = 91000: loss = 752790729129984.0\n",
            "step = 91000: Average Return = 0.0\n",
            "step = 91500: loss = 506412882984960.0\n",
            "step = 92000: loss = 3036792596987904.0\n",
            "step = 92000: Average Return = 100.0\n",
            "step = 92500: loss = 5338995462504448.0\n",
            "step = 93000: loss = 652292722262016.0\n",
            "step = 93000: Average Return = 14.0\n",
            "step = 93500: loss = 774042629963776.0\n",
            "step = 94000: loss = 551285963096064.0\n",
            "step = 94000: Average Return = 315.0\n",
            "step = 94500: loss = 352831932465152.0\n",
            "step = 95000: loss = 276531519684608.0\n",
            "step = 95000: Average Return = 100.0\n",
            "step = 95500: loss = 204518239240192.0\n",
            "step = 96000: loss = 147722296360960.0\n",
            "step = 96000: Average Return = 288.0\n",
            "step = 96500: loss = 53080863277056.0\n",
            "step = 97000: loss = 8942054801408.0\n",
            "step = 97000: Average Return = 242.0\n",
            "step = 97500: loss = 109249967423488.0\n",
            "step = 98000: loss = 470508109824.0\n",
            "step = 98000: Average Return = 169.0\n",
            "step = 98500: loss = 9294454980608.0\n",
            "step = 99000: loss = 1484271517696.0\n",
            "step = 99000: Average Return = 203.0\n",
            "step = 99500: loss = 230251102208.0\n",
            "step = 100000: loss = 44350345216.0\n",
            "step = 100000: Average Return = 46.0\n",
            "step = 100500: loss = 81469358080.0\n",
            "step = 101000: loss = 145278418944.0\n",
            "step = 101000: Average Return = 50.0\n",
            "step = 101500: loss = 1417675264.0\n",
            "step = 102000: loss = 3452290560.0\n",
            "step = 102000: Average Return = 287.0\n",
            "step = 102500: loss = 47850508288.0\n",
            "step = 103000: loss = 7930769408.0\n",
            "step = 103000: Average Return = 100.0\n",
            "step = 103500: loss = 46875664384.0\n",
            "step = 104000: loss = 70672924672.0\n",
            "step = 104000: Average Return = 100.0\n",
            "step = 104500: loss = 82480545792.0\n",
            "step = 105000: loss = 163168108544.0\n",
            "step = 105000: Average Return = 0.0\n",
            "step = 105500: loss = 154872152064.0\n",
            "step = 106000: loss = 149738455040.0\n",
            "step = 106000: Average Return = 0.0\n",
            "step = 106500: loss = 351361990656.0\n",
            "step = 107000: loss = 144236101632.0\n",
            "step = 107000: Average Return = 30.0\n",
            "step = 107500: loss = 8341589852160.0\n",
            "step = 108000: loss = 334044692480.0\n",
            "step = 108000: Average Return = 21.0\n",
            "step = 108500: loss = 152140431360.0\n",
            "step = 109000: loss = 317145546752.0\n",
            "step = 109000: Average Return = 12.0\n",
            "step = 109500: loss = 513667760128.0\n",
            "step = 110000: loss = 100143030272.0\n",
            "step = 110000: Average Return = 0.0\n",
            "step = 110500: loss = 928132628480.0\n",
            "step = 111000: loss = 1468722053120.0\n",
            "step = 111000: Average Return = 140.0\n",
            "step = 111500: loss = 850165366784.0\n",
            "step = 112000: loss = 2414100873216.0\n",
            "step = 112000: Average Return = 30.0\n",
            "step = 112500: loss = 213970944000.0\n",
            "step = 113000: loss = 3127861837824.0\n",
            "step = 113000: Average Return = 0.0\n",
            "step = 113500: loss = 7649403338752.0\n",
            "step = 114000: loss = 8900242833408.0\n",
            "step = 114000: Average Return = 100.0\n",
            "step = 114500: loss = 1157391843328.0\n",
            "step = 115000: loss = 79489318912.0\n",
            "step = 115000: Average Return = 100.0\n",
            "step = 115500: loss = 81931010048.0\n",
            "step = 116000: loss = 413680336896.0\n",
            "step = 116000: Average Return = 100.0\n",
            "step = 116500: loss = 1031032668160.0\n",
            "step = 117000: loss = 517345247232.0\n",
            "step = 117000: Average Return = 100.0\n",
            "step = 117500: loss = 1451304943616.0\n",
            "step = 118000: loss = 148624244736.0\n",
            "step = 118000: Average Return = 0.0\n",
            "step = 118500: loss = 64003366912.0\n",
            "step = 119000: loss = 123983962112.0\n",
            "step = 119000: Average Return = 30.0\n",
            "step = 119500: loss = 50344157184.0\n",
            "step = 120000: loss = 3469195608064.0\n",
            "step = 120000: Average Return = 100.0\n",
            "step = 120500: loss = 75483611136.0\n",
            "step = 121000: loss = 932710580224.0\n",
            "step = 121000: Average Return = 0.0\n",
            "step = 121500: loss = 255068258304.0\n",
            "step = 122000: loss = 1214618796032.0\n",
            "step = 122000: Average Return = 30.0\n",
            "step = 122500: loss = 148019609600.0\n",
            "step = 123000: loss = 188773040128.0\n",
            "step = 123000: Average Return = 140.0\n",
            "step = 123500: loss = 2152934408192.0\n",
            "step = 124000: loss = 64780226560.0\n",
            "step = 124000: Average Return = 30.0\n",
            "step = 124500: loss = 262698254336.0\n",
            "step = 125000: loss = 6531759210496.0\n",
            "step = 125000: Average Return = 140.0\n",
            "step = 125500: loss = 786428592128.0\n",
            "step = 126000: loss = 325750620160.0\n",
            "step = 126000: Average Return = 140.0\n",
            "step = 126500: loss = 240882384896.0\n",
            "step = 127000: loss = 289968685056.0\n",
            "step = 127000: Average Return = 0.0\n",
            "step = 127500: loss = 1371798634496.0\n",
            "step = 128000: loss = 183952474112.0\n",
            "step = 128000: Average Return = 0.0\n",
            "step = 128500: loss = 421655085056.0\n",
            "step = 129000: loss = 127183454208.0\n",
            "step = 129000: Average Return = 30.0\n",
            "step = 129500: loss = 261218959360.0\n",
            "step = 130000: loss = 108704579584.0\n",
            "step = 130000: Average Return = 30.0\n",
            "step = 130500: loss = 1704133263360.0\n",
            "step = 131000: loss = 281839730688.0\n",
            "step = 131000: Average Return = 100.0\n",
            "step = 131500: loss = 407108419584.0\n",
            "step = 132000: loss = 189372137472.0\n",
            "step = 132000: Average Return = 0.0\n",
            "step = 132500: loss = 6340861755392.0\n",
            "step = 133000: loss = 256404766720.0\n",
            "step = 133000: Average Return = 100.0\n",
            "step = 133500: loss = 433677598720.0\n",
            "step = 134000: loss = 22209078755328.0\n",
            "step = 134000: Average Return = 0.0\n",
            "step = 134500: loss = 158955454464.0\n",
            "step = 135000: loss = 510982356992.0\n",
            "step = 135000: Average Return = 0.0\n",
            "step = 135500: loss = 1761430732800.0\n",
            "step = 136000: loss = 357944655872.0\n",
            "step = 136000: Average Return = 140.0\n",
            "step = 136500: loss = 98711207936.0\n",
            "step = 137000: loss = 230669025280.0\n",
            "step = 137000: Average Return = 30.0\n",
            "step = 137500: loss = 34641264640.0\n",
            "step = 138000: loss = 1076397998080.0\n",
            "step = 138000: Average Return = 100.0\n",
            "step = 138500: loss = 1815699259392.0\n",
            "step = 139000: loss = 417377681408.0\n",
            "step = 139000: Average Return = 0.0\n",
            "step = 139500: loss = 549184471040.0\n",
            "step = 140000: loss = 649266069504.0\n",
            "step = 140000: Average Return = 30.0\n",
            "step = 140500: loss = 2198565814272.0\n",
            "step = 141000: loss = 161966276608.0\n",
            "step = 141000: Average Return = 9.0\n",
            "step = 141500: loss = 848527491072.0\n",
            "step = 142000: loss = 3718787366912.0\n",
            "step = 142000: Average Return = 0.0\n",
            "step = 142500: loss = 179130073088.0\n",
            "step = 143000: loss = 367488204800.0\n",
            "step = 143000: Average Return = 30.0\n",
            "step = 143500: loss = 94989484032.0\n",
            "step = 144000: loss = 1045102264320.0\n",
            "step = 144000: Average Return = 100.0\n",
            "step = 144500: loss = 176681484288.0\n",
            "step = 145000: loss = 94135902208.0\n",
            "step = 145000: Average Return = 6.0\n",
            "step = 145500: loss = 782063304704.0\n",
            "step = 146000: loss = 9130586669056.0\n",
            "step = 146000: Average Return = 0.0\n",
            "step = 146500: loss = 90934812672.0\n",
            "step = 147000: loss = 231751614464.0\n",
            "step = 147000: Average Return = 27.0\n",
            "step = 147500: loss = 366274084864.0\n",
            "step = 148000: loss = 3730162057216.0\n",
            "step = 148000: Average Return = 18.0\n",
            "step = 148500: loss = 267521146880.0\n",
            "step = 149000: loss = 44880396288.0\n",
            "step = 149000: Average Return = 30.0\n",
            "step = 149500: loss = 229290655744.0\n",
            "step = 150000: loss = 373374255104.0\n",
            "step = 150000: Average Return = 30.0\n",
            "step = 150500: loss = 83459268608.0\n",
            "step = 151000: loss = 205635289088.0\n",
            "step = 151000: Average Return = 30.0\n",
            "step = 151500: loss = 133383946240.0\n",
            "step = 152000: loss = 1230786002944.0\n",
            "step = 152000: Average Return = 30.0\n",
            "step = 152500: loss = 77923966976.0\n",
            "step = 153000: loss = 143628009472.0\n",
            "step = 153000: Average Return = 100.0\n",
            "step = 153500: loss = 464532799488.0\n",
            "step = 154000: loss = 135566204928.0\n",
            "step = 154000: Average Return = 0.0\n",
            "step = 154500: loss = 228565417984.0\n",
            "step = 155000: loss = 105663668224.0\n",
            "step = 155000: Average Return = 6.0\n",
            "step = 155500: loss = 84283891712.0\n",
            "step = 156000: loss = 210310479872.0\n",
            "step = 156000: Average Return = 140.0\n",
            "step = 156500: loss = 3656047656960.0\n",
            "step = 157000: loss = 730564984832.0\n",
            "step = 157000: Average Return = 18.0\n",
            "step = 157500: loss = 1294165213184.0\n",
            "step = 158000: loss = 1557061435392.0\n",
            "step = 158000: Average Return = 100.0\n",
            "step = 158500: loss = 8464027877376.0\n",
            "step = 159000: loss = 241925832704.0\n",
            "step = 159000: Average Return = 140.0\n",
            "step = 159500: loss = 82539773952.0\n",
            "step = 160000: loss = 32121034752.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgMU3hxoVsuT",
        "colab_type": "text"
      },
      "source": [
        "# **Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3skqbSLVwOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plot\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "805XaJ_EV1Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Video\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kDYPz2DZ_9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_spec = array_spec.BoundedArraySpec((), np.int64, 0, 0)\n",
        "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
        "    action_spec=action_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey_0uDymbcia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_policy_eval_video(my_random_py_policy, \"doomy-agent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYnC4fhDgRFa",
        "colab_type": "text"
      },
      "source": [
        "explore maually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWVFNwXRgT99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_data(train_env, random_policy, replay_buffer, steps=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6da1u-3gX0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iter(replay_buffer.as_dataset()).next()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}